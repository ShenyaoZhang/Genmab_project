# -*- coding: utf-8 -*-
"""APEtrainer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qzrxft_doyalVZwIUTcRr3Dng0SqzL2h
"""

!pip uninstall -y transformers datasets accelerate evaluate huggingface_hub
!pip install --no-cache-dir torch torchvision torchaudio
!pip install --no-cache-dir transformers==4.57.1 datasets accelerate evaluate

import transformers
print("Transformers version:", transformers.__version__)
print("Transformers file:", transformers.__file__)

# from transformers import TrainingArguments
# print("TrainingArguments object:", TrainingArguments)
# help(TrainingArguments)

import torch
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

from torch import nn
from transformers import Trainer
from transformers import DataCollatorWithPadding
import evaluate
from datasets import load_dataset
from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    Trainer,
    TrainingArguments
)
import pandas as pd
from datasets import load_dataset, DatasetDict

# 1️⃣ Load the ADE dataset
dataset = load_dataset("SetFit/ade_corpus_v2_classification")

# 2️⃣ Load your mislabeled data
mislabelled_df = pd.read_csv("ade_mislabeled_candidates.csv")
mislabelled_texts = set(mislabelled_df["text"].tolist())

# 3️⃣ Define a filter function
def filter_mislabelled(example):
    return example["text"] not in mislabelled_texts

# 4️⃣ Apply the filter to every split
filtered_dataset = {}
for split in dataset.keys():
    filtered_dataset[split] = dataset[split].filter(filter_mislabelled)
    print(f"{split}: {len(dataset[split])} → {len(filtered_dataset[split])} samples after filtering")

# ✅ GPU check
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

# Load dataset
dataset =  DatasetDict(filtered_dataset)


# Tokenizer + Model
tokenizer = BertTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
model = BertForSequenceClassification.from_pretrained("emilyalsentzer/Bio_ClinicalBERT", num_labels=2)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# def tokenize(batch):
#     return tokenizer(batch["text"], truncation=True, padding=True)

def tokenize(batch):
    return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=256)


dataset = dataset.map(tokenize, batched=True)
dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

# Define metrics

# acc = evaluate.load("accuracy")
# def compute_metrics(p):
#     preds = torch.argmax(torch.tensor(p.predictions), dim=-1)
#     return acc.compute(predictions=preds, references=p.label_ids)

acc = evaluate.load("accuracy")
f1 = evaluate.load("f1")
precision = evaluate.load("precision")
recall = evaluate.load("recall")

def compute_metrics(p):
    preds = torch.argmax(torch.tensor(p.predictions), dim=-1)
    return {
        "accuracy": acc.compute(predictions=preds, references=p.label_ids)["accuracy"],
        "precision": precision.compute(predictions=preds, references=p.label_ids)["precision"],
        "recall": recall.compute(predictions=preds, references=p.label_ids)["recall"],
        "f1": f1.compute(predictions=preds, references=p.label_ids)["f1"],
    }


# Training setup
args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=6,
    weight_decay=0.01,
    report_to="none"
)


#### weighted loss section ####

# 1️⃣ compute class weights from training labels
train_labels = dataset["train"]["label"]
class_counts = torch.bincount(torch.tensor(train_labels))
weights = 1.0 / class_counts.float()
weights = weights / weights.sum()
print("Class weights:", weights)

# 2️⃣ subclass Trainer to apply weights
class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")

        loss_fct = nn.CrossEntropyLoss(weight=weights.to(model.device))
        loss = loss_fct(
            logits.view(-1, self.model.config.num_labels),
            labels.view(-1)
        )

        return (loss, outputs) if return_outputs else loss

# 3️⃣ use the new trainer instead of plain Trainer
weighted_trainer = WeightedTrainer(
    model=model,
    args=args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,   # your padding collator
)

# 4️⃣ train normally
weighted_trainer.train()

###############################

# trainer = Trainer(
#     model=model,
#     args=args,
#     train_dataset=dataset["train"],
#     eval_dataset=dataset["test"],
#     compute_metrics=compute_metrics,
#     data_collator=data_collator,
# )

# trainer.train()

weighted_trainer.save_model("bio_ae_detector")
tokenizer.save_pretrained("bio_ae_detector")

text = "The patient experienced severe dizziness and nausea after taking Drug ibuprofen."
inputs = tokenizer(text, return_tensors="pt").to("cuda")
with torch.no_grad():
    logits = model(**inputs).logits
    probs = torch.nn.functional.softmax(logits, dim=-1)
print("Probabilities:", probs)
print("Predicted label:", torch.argmax(probs, dim=-1).item())

samples = [
    "The patient experienced severe dizziness and nausea after taking Drug X.",
    "Patient was given Drug X and felt fine.",
    "Patient continued regular therapy without complications.",
    "RESULTS: Evidence of neurological improvement and rehabilitation potential after severe myelopathy due to intrathecal injection of doxorubicin."
]

for s in samples:
    inputs = tokenizer(s, return_tensors="pt").to("cuda")
    with torch.no_grad():
        probs = torch.nn.functional.softmax(model(**inputs).logits, dim=-1)
    print(f"{s}\n→ label: {torch.argmax(probs).item()}, probs: {probs.cpu().numpy()}\n")

from datasets import load_dataset
from collections import Counter

ds = load_dataset("SetFit/ade_corpus_v2_classification")
train_labels = ds["train"]["label"]
test_labels = ds["test"]["label"]

print("Train label counts:", Counter(train_labels))
print("Test label counts:", Counter(test_labels))

!zip -r bio_ae_detector.zip bio_ae_detector

from google.colab import files
files.download("bio_ae_detector.zip")

