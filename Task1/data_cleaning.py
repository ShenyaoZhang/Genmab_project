# -*- coding: utf-8 -*-
"""Data_Cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Blw01BBcCoVYxULluDx40gVIY2GNxgB9
"""

from transformers import AutoTokenizer, AutoModel
from datasets import load_dataset
from tqdm import tqdm
import torch
import numpy as np

# ------------------------------
# 1Ô∏è‚É£ Device setup (GPU on Mac)
# ------------------------------
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("‚úÖ Using device: CUDA GPU")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    print("‚úÖ Using device: Apple MPS GPU")
else:
    device = torch.device("cpu")
    print("‚öôÔ∏è Using device: CPU (no GPU detected)")

# ------------------------------
# 2Ô∏è‚É£ Load dataset
# ------------------------------
ds = load_dataset("SetFit/ade_corpus_v2_classification")
train = ds["train"].to_pandas()

# ------------------------------
# 3Ô∏è‚É£ Load Bio_ClinicalBERT
# ------------------------------
tokenizer = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
model = AutoModel.from_pretrained("emilyalsentzer/Bio_ClinicalBERT").to(device)
model.eval()  # disable dropout etc.

# ------------------------------
# 4Ô∏è‚É£ Function to get embeddings (with progress bar)
# ------------------------------
def get_embeddings(texts, batch_size=16):
    embs = []
    for i in tqdm(range(0, len(texts), batch_size), desc="üîç Encoding batches"):
        batch = texts[i:i+batch_size]
        inputs = tokenizer(
            batch,
            padding=True,
            truncation=True,
            return_tensors="pt",
            max_length=128
        ).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            # Mean pooling across tokens
            emb = outputs.last_hidden_state.mean(dim=1)
        embs.append(emb.cpu())
    return torch.cat(embs).numpy()

# ------------------------------
# 5Ô∏è‚É£ Run and show progress
# ------------------------------
embs = get_embeddings(train["text"].tolist(), batch_size=16)

print("‚úÖ Embedding shape:", embs.shape)

np.save("ade_embeddings.npy", embs)
print("üíæ Saved embeddings to ade_embeddings.npy")

from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# Compute cosine similarity (this can be memory heavy ‚Äî do partial)
sims = cosine_similarity(embs)

# For each sample, compare with its top 5 nearest neighbors
labels = train["label"].values
mislabeled = []

for i in range(len(train)):
    nn_idx = np.argsort(-sims[i])[1:6]  # top 5 neighbors (skip itself)
    neighbor_labels = labels[nn_idx]
    # If most neighbors have different label ‚Üí flag as suspicious
    if np.sum(neighbor_labels == labels[i]) <= 2:
        mislabeled.append(i)

suspects = train.iloc[mislabeled]
print(f"‚ö†Ô∏è  Possible mislabeled samples: {len(suspects)} / {len(train)}")
print(suspects.head())

# Optionally save
suspects.to_csv("ade_mislabeled_candidates.csv", index=False)
print("üíæ Saved suspicious samples to ade_mislabeled_candidates.csv")

from huggingface_hub import login
login()

from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# model_id = "meta-llama/Llama-2-7b-chat-hf"
model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

print(f"Loading tokenizer for {model_id} ...")
tokenizer = AutoTokenizer.from_pretrained(model_id, token=True)

print(f"Loading model {model_id} in fp16 (no quantization) ...")
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,   # half precision
    device_map="auto",           # automatically puts layers on GPU
    token=True
)

print(f"{model_id} loaded successfully ‚úÖ")

def ask_llama(text, label):
    prompt = f"""[INST] <<SYS>>
    You are a strict classifier.
    Respond exactly with two lines:
    Line 1: Correct / Incorrect / Unclear
    Line 2: Short reason (‚â§15 words).
    <</SYS>>
    Text: "{text}"
    Assigned label: "{label}"
    Possible labels: ADE, Not-Related.
    Does the label correctly describe the text? [/INST]
    """
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=80)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# --- 5Ô∏è‚É£ Run on Sample Rows ---------------------------------------------------
samples = train.sample(5, random_state=0)
for _, row in samples.iterrows():
    print(ask_llama(row["text"], row["label_text"]))
    print("‚Äî"*60)

# --- 6Ô∏è‚É£ (Optional) Save Results ---------------------------------------------
samples["llama_judgment"] = samples.apply(
    lambda r: ask_llama(r["text"], r["label_text"]),
    axis=1
)
samples.to_csv("llama_reviewed.csv", index=False)
print("‚úÖ Saved results to llama_reviewed.csv")

!pip install gradio

import gradio as gr
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --- Load model ---
model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

# --- Define classifier function ---
def classify_text(text, label):
    prompt = f"""[INST] <<SYS>>
    You are a strict classifier. Respond with 'Correct', 'Incorrect', or 'Unclear'
    and one short reason (‚â§15 words).
    <</SYS>>
    Text: "{text}"
    Assigned label: "{label}"
    Possible labels: ADE, Not-Related.
    Does the label correctly describe the text? [/INST]"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=60)
    raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # --- cleaning ---
    cleaned = raw_output.replace("[INST]", "").replace("[/INST]", "")
    cleaned = cleaned.replace("<<SYS>>", "").replace("<</SYS>>", "").strip()
    import re
    match = re.search(r"(Correct|Incorrect|Unclear)[\s\S]*", cleaned)
    if match:
        cleaned = match.group(0).strip()

    return cleaned


# --- Gradio UI ---
demo = gr.Interface(
    fn=classify_text,
    inputs=[
        gr.Textbox(label="Text", placeholder="Enter a medical description..."),
        gr.Radio(["ADE", "Not-Related"], label="Assigned Label")
    ],
    outputs="text",
    title="üîç ADE Label Verifier (LLaMA-3-8B)",
    description="Check whether an ADE label fits the input text."
)

demo.launch(share=True)

# Test directly in notebook
try:
    print(classify_text(
        "Complex biochemical syndrome of hypocalcemia and hypoparathyroidism during cytotoxic treatment of an infant with leukemia.",
        "Not-Related"
    ))
except Exception as e:
    import traceback
    traceback.print_exc()

import requests
import pandas as pd
from tqdm import tqdm

API_KEY = "bd00d902-ef40-4d7a-bd48-5d071830ce02"
BASE_URL = "https://data.bioontology.org/ontologies/MEDDRA/classes"

def fetch_meddra_terms(max_pages=5, page_size=100):
    """Fetch MedDRA terms from BioPortal"""
    headers = {"Authorization": f"apikey token={API_KEY}"}
    terms = []

    for page in tqdm(range(1, max_pages+1)):
        url = f"{BASE_URL}?page={page}&pagesize={page_size}"
        response = requests.get(url, headers=headers)
        data = response.json()

        for entry in data.get("collection", []):
            terms.append({
                "term": entry.get("prefLabel"),
                "definition": entry.get("definition", [""])[0] if entry.get("definition") else "",
                "uri": entry.get("@id")
            })

        if "nextPage" not in data.get("links", {}):
            break

    df = pd.DataFrame(terms).dropna(subset=["term"])
    return df

meddra_df = fetch_meddra_terms(max_pages=10)  # e.g. ~1000 terms
print(meddra_df.head())

# ------------------------------
# 3Ô∏è‚É£ Load Bio_ClinicalBERT + Fix padding issue
# ------------------------------
tokenizer = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")

# ‚úÖ Fix for missing pad token
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

model = AutoModel.from_pretrained("emilyalsentzer/Bio_ClinicalBERT").to(device)
model.resize_token_embeddings(len(tokenizer))
model.eval()  # disable dropout etc.

# # ------------------------------
# # 4Ô∏è‚É£ Embedding function
# # ------------------------------
def get_embeddings(texts, batch_size=16):
    embs = []
    for i in tqdm(range(0, len(texts), batch_size), desc="üîç Encoding batches"):
        batch = texts[i:i+batch_size]
        inputs = tokenizer(
            batch,
            padding=True,
            truncation=True,
            return_tensors="pt",
            max_length=128
        ).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            emb = outputs.last_hidden_state.mean(dim=1)
        embs.append(emb.cpu())
    return torch.cat(embs).numpy()

# ------------------------------
# 6Ô∏è‚É£ Generate embeddings for MedDRA terms
# ------------------------------
meddra_emb = get_embeddings(meddra_df["term"].astype(str).tolist(), batch_size=16)
np.save("meddra_embeddings_bioclinicalbert.npy", meddra_emb)
print("‚úÖ MedDRA embeddings generated and saved:", meddra_emb.shape)

# ------------------------------
# 7Ô∏è‚É£ Mapping function: find nearest MedDRA term
# ------------------------------
from sklearn.metrics.pairwise import cosine_similarity

def map_to_meddra(event_text, top_k=3):
    event_emb = get_embeddings([event_text])
    sims = cosine_similarity(event_emb, meddra_emb)[0]
    top_idx = np.argsort(sims)[::-1][:top_k]
    results = meddra_df.iloc[top_idx].copy()
    results["similarity"] = sims[top_idx]
    return results

examples = [
    "rash and fever after taking amoxicillin",
    "nausea and vomiting following chemotherapy",
    "shortness of breath and dizziness"
]

for text in examples:
    print(f"\nüßæ Event text: {text}")
    print(map_to_meddra(text))