#!/usr/bin/env python3
"""
Task 5 - Step 3/9: Data preprocessing (simplified)

Preprocess the dataset generated by 01_extract_data.py.
"""

import sys
import pandas as pd
import numpy as np
import os
import json
import time
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from pathlib import Path


def preprocess_file(input_path, output_dir=".", verbose=True):
    """
    Preprocess data file with feature engineering and train-test splitting.

    Parameters:
        -----------
    input_path : str
    Path to input CSV file (e.g., "main_data.csv")
    output_dir : str
    Output directory for preprocessed files (default: ".")
    verbose : bool
    Whether to print progress messages (default: True)

    Returns:
        --------
    dict : Dictionary with paths to output files and metadata
    """
    if verbose:
        print(f" Input file: {input_path}")
        print()

    # Load data
    if verbose:
        print(" Loading source data...")
        df = pd.read_csv(input_path)
        if verbose:
            print(f" Raw data: {len(df)} rows, {len(df.columns)} columns")
    print()

    # Deduplicate on safetyreportid
    if 'safetyreportid' in df.columns:
        before_dedup = len(df)
        df = df.drop_duplicates(subset='safetyreportid', keep='first')
        after_dedup = len(df)
        duplicates_removed = before_dedup - after_dedup

        if duplicates_removed > 0:
            if verbose:
                print(f"WARNING: Deduplication removed {duplicates_removed} duplicate rows ({duplicates_removed / before_dedup * 100:.1f}%)")
        print(f" Remaining unique reports: {after_dedup}")
        else:

            if verbose:
                print(" Deduplication check: no duplicates found")
        if verbose:
            print()

    # Preprocessing pipeline
    if verbose:
        print("=" * 80)
        print(" Running preprocessing pipeline")
        print("=" * 80)
        print()
        print("Processing steps:")
        print("1. Convert numeric fields")
        print("2. Feature engineering (create new features)")
        print("3. Handle missing values")
        print("4. Clean data")
        print()

    # Step 1: convert numeric fields
        if verbose:
            print("1. Converting numeric fields...")

    # Convert severity indicators to numeric
    severity_cols = [
        'serious',
        'seriousnessdeath',
        'seriousnesshospitalization',
        'seriousnesslifethreatening',
        'seriousnessdisabling',
        'seriousnesscongenitalanomali',
        'seriousnessother']

    for col in severity_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
    # Convert to binary: treat any non-zero value as 1
    df[col] = (df[col] > 0).astype(int)

    # Convert patient attributes
    if 'patientsex' in df.columns:
        df['patientsex'] = pd.to_numeric(
            df['patientsex'], errors='coerce').fillna(0).astype(int)

        if 'patientonsetage' in df.columns:
            df['patientonsetage'] = pd.to_numeric(
        df['patientonsetage'], errors='coerce')

    if 'patientweight' in df.columns:
        df['patientweight'] = pd.to_numeric(
            df['patientweight'], errors='coerce')

    # Convert drug and reaction counts
        if 'num_drugs' in df.columns:
            df['num_drugs'] = pd.to_numeric(
        df['num_drugs'],
        errors='coerce').fillna(1).astype(int)

    if 'num_reactions' in df.columns:
        df['num_reactions'] = pd.to_numeric(
            df['num_reactions'], errors='coerce').fillna(1).astype(int)

        if verbose:
            print(" Numeric conversion complete")
    print()

    original_cols_count = len(df.columns)

    # Step 2: feature engineering
    if verbose:
        print("2. Feature engineering...")

    # Build age-related features and handle outliers
        if 'patientonsetage' in df.columns:
            # Treat out-of-range ages (<0 or >120) as missing
    df.loc[df['patientonsetage'] > 120, 'patientonsetage'] = np.nan
    df.loc[df['patientonsetage'] < 0, 'patientonsetage'] = np.nan

    # Age buckets
    df['age_group'] = pd.cut(
        df['patientonsetage'],
        bins=[0, 18, 45, 65, 120],
        labels=['0-18', '19-45', '46-65', '66+'],
        include_lowest=True,
    )

    # One-hot encode the age buckets
    age_dummies = pd.get_dummies(df['age_group'], prefix='age')
    df = pd.concat([df, age_dummies], axis=1)

    # Flag missing age values
    df['age_missing'] = df['patientonsetage'].isna().astype(int)

    # Build gender features
    if 'patientsex' in df.columns:
        df['sex_male'] = (df['patientsex'] == 1).astype(int)
        df['sex_female'] = (df['patientsex'] == 2).astype(int)
        df['sex_unknown'] = (df['patientsex'] == 0).astype(int)

    # Age threshold features
        if 'patientonsetage' in df.columns or 'age_years' in df.columns:
            age_col = 'age_years' if 'age_years' in df.columns else 'patientonsetage'
    if age_col in df.columns:
        df['age_gt_65'] = (df[age_col] > 65).astype(int)
        df['age_gt_70'] = (df[age_col] > 70).astype(int)
        df['age_gt_75'] = (df[age_col] > 75).astype(int)

    # Weight normalization and BMI features
    # CONTINUOUS VARIABLE PROCESSING:
    # - Weight (patientweight): Kept as continuous in original units (kg), no scaling needed for tree-based models.
    # Missing values filled with median, and a weight_missing flag is created.
    # - BMI: Calculated from weight (assuming default height if not available), kept as continuous feature.
    # - BMI buckets: Created as binary indicators (<18.5 underweight, 18.5-25 normal, 25-30 overweight, >30 obese).
    # - Age: Used both as continuous (age_years) and as binary indicators (age_gt_65, age_gt_70, age_gt_75).
    # - Polypharmacy: Captured both by continuous count (num_drugs) and by binary indicators (high_polypharmacy, etc.).
        if 'patientweight' in df.columns:
            if verbose:
                print(" Weight normalization and BMI calculation...")
    # Weight: Keep as continuous in original units (kg), no scaling needed for tree-based models
    # Missing values: Fill with median, create missing indicator flag
        weight = pd.to_numeric(df['patientweight'], errors='coerce')
        df['weight_missing'] = weight.isna().astype(int)
        median_weight = weight.median()
        df['patientweight'] = weight.fillna(median_weight)

    # BMI: Calculate from weight (assuming average height ~1.65m if height not available)
    # BMI is kept as a continuous feature for model use
        height_default = 1.65  # meters
        df['bmi'] = df['patientweight'] / (height_default ** 2)

    # BMI buckets: Create binary indicators based on thresholds
    # Underweight: <18.5, Normal: 18.5-25, Overweight: 25-30, Obese: >30
        df['bmi_underweight'] = (df['bmi'] < 18.5).astype(int)
        df['bmi_normal'] = ((df['bmi'] >= 18.5) & (df['bmi'] < 25)).astype(int)
        df['bmi_overweight'] = (
            (df['bmi'] >= 25) & (
                df['bmi'] < 30)).astype(int)
        df['bmi_obese'] = (df['bmi'] >= 30).astype(int)

        if verbose:
            print(f" BMI calculated for {df['patientweight'].notna().sum()} patients")
    print(
        f" Underweight: df['bmi_underweight'].sum()}, Normal: {df['bmi_normal'].sum()}")
    print(
        f" Overweight: df['bmi_overweight'].sum()}, Obese: {df['bmi_obese'].sum()}")

    # Multi-drug usage features (polypharmacy class-level)
    if 'num_drugs' in df.columns:
        df['polypharmacy'] = (df['num_drugs'] > 1).astype(int)
        df['high_polypharmacy'] = (df['num_drugs'] > 5).astype(int)
        df['moderate_polypharmacy'] = (
            (df['num_drugs'] >= 2) & (
                df['num_drugs'] <= 5)).astype(int)
        df['very_high_polypharmacy'] = (df['num_drugs'] > 10).astype(int)

    # Classify polypharmacy levels
        def classify_polypharmacy(num):
            if num <= 1:
                return 'low'
                elif num <= 5:
                    return 'moderate'
            elif num <= 10:
                return 'high'
        else:

            return 'very_high'

    df['polypharmacy_class'] = df['num_drugs'].apply(classify_polypharmacy)
    # One-hot encode polypharmacy class
    poly_dummies = pd.get_dummies(
        df['polypharmacy_class'],
        prefix='polypharmacy')
    df = pd.concat([df, poly_dummies], axis=1)

    # Drug class features (from all_drugs)
    if 'all_drugs' in df.columns:
        if verbose:
            print(" Extracting drug class features...")
    drugs_upper = df['all_drugs'].fillna('').str.upper()

    # Steroids
    df['has_steroid'] = (
        drugs_upper.str.contains(
            'PREDNISONE|PREDNISOLONE|DEXAMETHASONE|METHYLPREDNISOLONE|HYDROCORTISONE',
            na=False)).astype(int)

    # Antibiotics
    df['has_antibiotic'] = (
        drugs_upper.str.contains(
            'CIPROFLOXACIN|LEVOFLOXACIN|VANCOMYCIN|AMOXICILLIN|CEPHALEXIN|AZITHROMYCIN',
            na=False)).astype(int)

    # Antivirals
    df['has_antiviral'] = (
        drugs_upper.str.contains(
            'ACYCLOVIR|GANCICLOVIR|OSELTAMIVIR|VALACYCLOVIR',
            na=False)).astype(int)

    # Chemotherapy
    df['has_chemo'] = (
        drugs_upper.str.contains(
            'CYCLOPHOSPHAMIDE|DOXORUBICIN|GEMCITABINE|CISPLATIN|PACLITAXEL|CARBOPLATIN',
            na=False)).astype(int)

    # Targeted therapy
    df['has_targeted'] = (
        drugs_upper.str.contains(
            'RITUXIMAB|BRENTUXIMAB|LENALIDOMIDE|POMALIDOMIDE',
            na=False)).astype(int)

    # Antifungals
    df['has_antifungal'] = (
        drugs_upper.str.contains(
            'FLUCONAZOLE|VORICONAZOLE|AMPHOTERICIN|POSACONAZOLE',
            na=False)).astype(int)

    # Interaction terms (Sky's requirement)
    df['steroid_plus_antibiotic'] = (
        (df['has_steroid'] == 1) & (df['has_antibiotic'] == 1)
    ).astype(int)

    if verbose:
        print(
            f" Steroid: df['has_steroid'].sum()}, Antibiotic: {df['has_antibiotic'].sum()}")
        print(
            f" Antiviral: df['has_antiviral'].sum()}, Chemo: {df['has_chemo'].sum()}")
        print(
            f" Steroid+Antibiotic combo: {df['steroid_plus_antibiotic'].sum()}")

    # Reaction count features
        if 'num_reactions' in df.columns:
            df['multiple_reactions'] = (df['num_reactions'] > 1).astype(int)
    df['many_reactions'] = (df['num_reactions'] > 3).astype(int)

    # Extract Cancer Stage from drug_indication
    # NOTE: FAERS does NOT have a standardized cancer stage field.
    # DLBCL stage is not available as a structured variable in FAERS.
    # We attempt to extract stage information from free-text drug_indication field,
    # but this is imperfect and may miss many cases.
    #
    # For future datasets with structured stage data, a numeric cancer_stage field (1-4)
    # can be directly used. Our pipeline is designed to accept such a field.
    #
    # If a structured cancer_stage field becomes available in the future:
    # - Use it directly as: df['cancer_stage'] = df['cancer_stage'] # numeric 1-4
    # - Create derived features: df['advanced_stage'] = (df['cancer_stage'] >= 3).astype(int)
    # - Missing values will be automatically handled by the feature selection logic

    # Reserve a placeholder for structured stage data (currently None/NaN)
    if 'cancer_stage' not in df.columns:
        # Placeholder for future structured stage data
        df['cancer_stage'] = np.nan

        if 'drug_indication' in df.columns:
            if verbose:
                print(" Extracting cancer stage from free-text drug_indication (imperfect extraction)...")
        indication_upper = df['drug_indication'].fillna('').str.upper()

    # Extract cancer stage (Stage I, II, III, IV) from free-text
    # Match patterns like: "STAGE I", "STAGE 1", "STAGE II", "STAGE 2", "STAGE III", "STAGE 3", "STAGE IV", "STAGE 4"
    # LIMITATION: This is text pattern matching and may not capture all stage
    # information

    # Initialize all stages as 0
        df['cancer_stage_I'] = 0
        df['cancer_stage_II'] = 0
        df['cancer_stage_III'] = 0
        df['cancer_stage_IV'] = 0

    # Stage I/1
        stage_i_pattern = r'STAGE\s+[I1](\s|$|,|\|)'
        df['cancer_stage_I'] = indication_upper.str.contains(
            stage_i_pattern, regex=True, na=False).astype(int)

    # Stage II/2
        stage_ii_pattern = r'STAGE\s+[I2]{2}{\s|$|,|\|)'  # II or 2
        df['cancer_stage_II'] = indication_upper.str.contains(
            stage_ii_pattern, regex=True, na=False).astype(int)
    # Also check for "STAGE 2" separately
        df['cancer_stage_II'] = (
            df['cancer_stage_II'] | indication_upper.str.contains(
                r'STAGE\s+2(\s|$|,|\|)',
                regex=True,
                na=False)).astype(int)

    # Stage III/3
        stage_iii_pattern = r'STAGE\s+[I3]{3}{\s|$|,|\|)'  # III or 3
        df['cancer_stage_III'] = indication_upper.str.contains(
            stage_iii_pattern, regex=True, na=False).astype(int)
    # Also check for "STAGE 3" separately
        df['cancer_stage_III'] = (
            df['cancer_stage_III'] | indication_upper.str.contains(
                r'STAGE\s+3(\s|$|,|\|)', regex=True, na=False)).astype(int)

    # Stage IV/4
        stage_iv_pattern = r'STAGE\s+[IV4](\s|$|,|\|)'  # IV or 4
        df['cancer_stage_IV'] = indication_upper.str.contains(
            stage_iv_pattern, regex=True, na=False).astype(int)
    # Also check for "STAGE 4" separately
        df['cancer_stage_IV'] = (
            df['cancer_stage_IV'] | indication_upper.str.contains(
                r'STAGE\s+4(\s|$|,|\|)',
                regex=True,
                na=False)).astype(int)

    # Print statistics
        if verbose:
            print(f" Stage I: {df['cancer_stage_I'].sum()} patients")
    print(f" Stage II: {df['cancer_stage_II'].sum()} patients")
    print(f" Stage III: {df['cancer_stage_III'].sum()} patients")
    print(f" Stage IV: {df['cancer_stage_IV'].sum()} patients")

    # Create combined cancer_stage categorical feature (for analysis)
    def get_stage(row):
        if row['cancer_stage_IV'] == 1:
            return 'IV'
            elif row['cancer_stage_III'] == 1:
                return 'III'
                elif row['cancer_stage_II'] == 1:
                    return 'II'
            elif row['cancer_stage_I'] == 1:
                return 'I'
        else:

            return 'Unknown'

    df['cancer_stage'] = df.apply(get_stage, axis=1)

    # Missing value imputation for numeric features
    if verbose:
        print(" Handling missing values...")
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col not in ['seriousnessdeath', 'serious', 'seriousnesshospitalization',
                   'seriousnesslifethreatening', 'seriousnessdisabling',
                   'seriousnesscongenitalanomali', 'seriousnessother']:
                       if df[col].isna().sum() > 0:
                           median_val = df[col].median()
        if pd.notna(median_val):
            df[col] = df[col].fillna(median_val)
    if verbose:
        print(f" Filled {col} missing values with median: {median_val:.2f}")

    # Reminder: do not recreate the severity_score feature (introduces leakage)
        if verbose:
            print(" Feature engineering complete")
    print(f" New features added: {len(df.columns) - original_cols_count}")
    print()

    # Save snapshot of the preprocessed data
    preprocessed_path = os.path.join(output_dir, "preprocessed_data.csv")
    if verbose:
        print(" Saving preprocessed data...")
        df.to_csv(preprocessed_path, index=False)
        if verbose:
            print(f" Saved: {preprocessed_path}")
    print()

    # Display new feature names
    if verbose:
        print("ðŸ†• New feature list:")
        original_cols = set(pd.read_csv(input_path).columns)
        new_cols = [col for col in df.columns if col not in original_cols]
        for i, col in enumerate(new_cols, 1):
            print(f" {i:2d}. {col}")
    print()

    # Step 3: prepare training data
    if verbose:
        print(" Preparing training data")
        print("Target variable: seriousnessdeath (death indicator)")

        y = pd.to_numeric(
            df['seriousnessdeath'],
            errors='coerce').fillna(0).astype(int)
        positive_count = int(y.sum())
        negative_count = int(len(y) - positive_count)

    # Target label overview
        if verbose:
            print("Target distribution:")
    print(
        f" Positive (death): {positive_count}{ positive_count /
            len(y) *
            100:.1f}%)")
    print(
        f" Negative (survival): {negative_count}{ negative_count /
            len(y) *
            100:.1f}%)")

    # Select feature columns
    if verbose:
        print(" Selecting features...")

    # Fields to exclude from modelling
        exclude_cols = [
            'safetyreportid',
            'receivedate',  # raw date string
            'target_drug',  # drug name (text)
            'drugname',  # alternate drug name (text)
            'all_drugs',  # list of drugs (text)
            'drug_indication',  # indication text
            'reactions',  # reaction list
            'patientonsetageunit',  # original age unit
            'age_group',  # bucket already expanded via one-hot encoding
            'reporter_qualification',  # reporter qualification (text)
            # Target labels and related severity flags
            'serious',
            'seriousnessdeath',
            'seriousnesshospitalization',
            'seriousnesslifethreatening',
            'seriousnessdisabling',
            'seriousnesscongenitalanomali',
            'seriousnessother',
        ]

    # Filter numeric features using robust dtype checks
        feature_cols = [
            col for col in df.columns if col not in exclude_cols and np.issubdtype(
                df[col].dtype, np.number)]

    X = df[feature_cols].copy()
    class_counts = y.value_counts()
    min_class_count = int(class_counts.min())

    # Safety check: ensure no leakage columns are present
    leakage_cols = [col for col in feature_cols if col.startswith(
        'seriousness') or col == 'serious']
    if leakage_cols:
        raise AssertionError(
            f"ERROR: Data leakage! Exclude columns: {leakage_cols}")
        if verbose:
            print(" Leakage check passed: no severity-related columns present")
    print(f" Selected {len(feature_cols)} features")
    print("Feature columns:")
    for i, col in enumerate(feature_cols, 1):
        print(f" {i:2d}. {col}")
        print()

    # Step 4: handle missing values
        if verbose:
            print(" Handling missing values...")
    print(" Missing value summary:")
    missing_counts = X.isnull().sum()
    missing_cols = missing_counts[missing_counts > 0]
    if len(missing_cols) > 0:
        if verbose:
            for col, count in missing_cols.items():
                pct = count / len(X) * 100
        print(f" {col}: {count}{pct:.1f}%)")
        else:

            if verbose:
                print(" None")

    # Impute with median
        imputer = SimpleImputer(strategy='median')
        X_imputed = pd.DataFrame(
            imputer.fit_transform(X),
            columns=X.columns,
            index=X.index
        )
        if verbose:
            print(" Missing value handling complete")
    print()

    # Step 5: split train/test sets
    if verbose:
        print(" Splitting train and test sets...")

    # Fallback if stratified split is not feasible
        if min_class_count >= 2:
            # Stratified split when both classes have >=2 samples
    X_train, X_test, y_train, y_test = train_test_split(
        X_imputed, y,
        test_size=0.2,
        random_state=42,
        stratify=y,
    )
    if verbose:
        print("Using stratified split (stratify=y)")
        else:

            # Class counts too small, fallback to unstratified split
    if verbose:
        print(
            f"WARNING: Warning: minimum class size {min_class_count}; using non-stratified split")
        X_train, X_test, y_train, y_test = train_test_split(
            X_imputed, y,
            test_size=0.2,
            random_state=42,
            stratify=None,
        )

        if verbose:
            print(f"Train set: {len(X_train)} samples")
    print(f"Test set: {len(X_test)} samples")

    # Safety check: ensure no shared IDs between train/test
    if 'safetyreportid' in df.columns:
        ids_train = set(df.loc[X_train.index, 'safetyreportid'])
        ids_test = set(df.loc[X_test.index, 'safetyreportid'])
        inter = ids_train & ids_test
        if len(inter) > 0:
            if verbose:
                print(
            f"ERROR: Warning: ID leakage between train/test! Example IDs: {list(inter)[:5]}")
        else:

            if verbose:
                print(" ID leakage check passed: no shared IDs")

        train_pos = int(y_train.sum())
        train_neg = int(len(y_train) - train_pos)
        test_pos = int(y_test.sum())
        test_neg = int(len(y_test) - test_pos)

        if verbose:
            print("Train target distribution:")
    print(f" Positive: {train_pos}{train_pos / len(y_train) * 100:.1f}%)")
    print(f" Negative: {train_neg}{train_neg / len(y_train) * 100:.1f}%)")

    print("Test target distribution:")
    print(f" Positive: {test_pos}{test_pos / len(y_test) * 100:.1f}%)")
    print(f" Negative: {test_neg}{test_neg / len(y_test) * 100:.1f}%)")

    # Save datasets
    if verbose:
        print(" Saving train and test datasets...")
        X_train_path = os.path.join(output_dir, "X_train.csv")
        y_train_path = os.path.join(output_dir, "y_train.csv")
        X_test_path = os.path.join(output_dir, "X_test.csv")
        y_test_path = os.path.join(output_dir, "y_test.csv")

        X_train.to_csv(X_train_path, index=False)
        y_train.to_csv(y_train_path, index=False, header=['seriousnessdeath'])
        X_test.to_csv(X_test_path, index=False)
        y_test.to_csv(y_test_path, index=False, header=['seriousnessdeath'])

        if verbose:
            print(" Saved:")
    print(f" - {X_train_path}")
    print(f" - {y_train_path}")
    print(f" - {X_test_path}")
    print(f" - {y_test_path}")

    # Save preprocessing metadata
    preprocess_meta = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "input_records": len(df),
        "output_records": len(df),
        "duplicates_removed": 0,  # handled during deduplication
        "target_variable": "seriousnessdeath",
        "positive_count": int(y.sum()),
        "negative_count": int(len(y) - y.sum()),
        "positive_rate": float(y.mean()),
        "negative_rate": float(1 - y.mean()),
        "train_size": len(X_train),
        "test_size": len(X_test),
        "train_pos": train_pos,
        "train_neg": train_neg,
        "test_pos": test_pos,
        "test_neg": test_neg,
        "num_features": len(feature_cols),
        "feature_cols": feature_cols,
        "excluded_cols": exclude_cols,
        "random_state": 42,
        "stratified": min_class_count >= 2
    }

    meta_path = os.path.join(output_dir, "preprocess_meta.json")
    with open(meta_path, "w", encoding='utf-8') as f:
        json.dump(preprocess_meta, f, indent=2, ensure_ascii=False)

        if verbose:
            print(" - preprocess_meta.json")
    print()

    # Show sample rows
    print("=" * 80)
    print(" Sample of preprocessed data")
    print("=" * 80)
    print()
    print(df[['target_drug', 'seriousnessdeath', 'patientonsetage',
          'patientsex', 'num_drugs', 'num_reactions']].head(3))
    print()

    # Summary
    print("=" * 80)
    print(" Step 3 complete - preprocessing finished")
    print("=" * 80)
    print()

    print(" Generated files:")
    print(f" 1. {preprocessed_path} - preprocessed dataset")
    print(f" 2. {X_train_path} - training features")
    print(f" 3. {y_train_path} - training labels")
    print(f" 4. {X_test_path} - test features")
    print(f" 5. {y_test_path} - test labels")
    print()

    # Return output file paths
    return {
        'preprocessed_data': preprocessed_path,
        'X_train': X_train_path,
        'y_train': y_train_path,
        'X_test': X_test_path,
        'y_test': y_test_path,
        'metadata': meta_path,
        'feature_cols': feature_cols
    }


# Main execution (for backward compatibility)
if __name__ == '__main__':
    # Locate input file
    DATA_FILES = ["main_data.csv", "oncology_drugs_complete.csv",
                  "oncology_drugs_data.csv", "epcoritamab_data.csv"]
    DATA_FILE = None

    for f in DATA_FILES:
        if os.path.exists(f):
            DATA_FILE = f
        break

        if DATA_FILE is None:
            print("ERROR: Error: data file not found")
    print()
    print("Please run: python 01_extract_data.py")
    sys.exit(1)

    print(f" Input file: {DATA_FILE}")
    if DATA_FILE == "main_data.csv":
        print(" (Task 5 dataset - 35 oncology drugs)")
                elif DATA_FILE == "oncology_drugs_complete.csv":
                    print(" (Complete oncology dataset)")
            elif DATA_FILE == "oncology_drugs_data.csv":
                print(" (Multi-drug oncology dataset)")
        else:

            print(" (Single-drug dataset)")
    print()

    # Run preprocessing
    results = preprocess_file(DATA_FILE, output_dir=".", verbose=True)

    print(" Next steps:")
    print(" Run: python 04_train_models.py")
    print(" Purpose: train multiple machine learning models")
    print()
